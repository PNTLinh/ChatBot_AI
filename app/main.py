import streamlit as st
from components import render_header, render_answer
<<<<<<< HEAD
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import google.generativeai as genai
import re
import time

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="TTHC RAG System - Gemini Flash",
    layout="wide",
)

render_header()

# Sidebar config
with st.sidebar:
    st.header("ðŸ¤– Gemini 1.5 Flash")
    gemini_key = st.text_input(
        "Google API Key:", 
        type="password",
        help="Láº¥y API key táº¡i: https://aistudio.google.com/app/apikey"
    )
    
    # Advanced settings
    with st.expander("âš™ï¸ CÃ i Ä‘áº·t nÃ¢ng cao"):
        temperature = st.slider("Temperature (Äá»™ sÃ¡ng táº¡o)", 0.0, 1.0, 0.1, 0.1)
        max_tokens = st.slider("Max tokens", 100, 2000, 800, 100)
        top_k = st.slider("Sá»‘ tÃ i liá»‡u tham kháº£o", 2, 8, 4, 1)

# 1) Form nháº­p cÃ¢u há»i
with st.form("query_form"):
    query = st.text_input("Nháº­p cÃ¢u há»i vá» thá»§ tá»¥c hÃ nh chÃ­nh")
    col1, col2 = st.columns(2)
    with col1:
        submitted = st.form_submit_button("TÃ¬m kiáº¿m", use_container_width=True)
    with col2:
        detailed = st.form_submit_button("Tráº£ lá»i chi tiáº¿t", use_container_width=True)

if not (submitted or detailed) or not query:
    st.stop()

# Kiá»ƒm tra API key
if not gemini_key:
    st.error(" Vui lÃ²ng nháº­p Google API Key trong sidebar")
    st.stop()

# 2) Setup Gemini
@st.cache_resource
def setup_gemini(api_key):
    """Setup Gemini model with caching"""
    genai.configure(api_key=api_key)
    
    # Cáº¥u hÃ¬nh generation
    generation_config = {
        "temperature": temperature,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": max_tokens,
    }
    
    # Safety settings (tÃ¹y chá»n)
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    
    return genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=generation_config,
        safety_settings=safety_settings
    )

# 3) Load vectorstore
@st.cache_resource(ttl=86400)
def load_vectorstore():
    embed = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
=======
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="TTHC RAG System",
    layout="wide",
)

# Header
render_header()

# 1) Form nháº­p cÃ¢u há»i
with st.form("query_form"):
    query = st.text_input("ðŸ”Ž Nháº­p cÃ¢u há»i cá»§a báº¡n")
    submitted = st.form_submit_button("TÃ¬m kiáº¿m")

# Dá»«ng náº¿u chÆ°a submit hoáº·c query rá»—ng
if not submitted or not query:
    st.stop()

# 2) Lazy-load vectorstore tá»« disk
@st.cache_resource(ttl=86400)
def load_vectorstore():
    embed = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
>>>>>>> e10216675b09a9e6e5fe87885184f76e4f60b7a1
    return FAISS.load_local(
        "data/faiss_store",
        embed,
        allow_dangerous_deserialization=True,
    )

<<<<<<< HEAD
# 4) Optimized prompts cho Gemini Flash
def get_prompt_template(is_detailed=False):
    """Get optimized prompt template for Gemini Flash"""
    
    if is_detailed:
        return """
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n thá»§ tá»¥c hÃ nh chÃ­nh Viá»‡t Nam vá»›i kinh nghiá»‡m 10 nÄƒm.

THÃ”NG TIN THAM KHáº¢O:
{context}

CÃ‚U Há»ŽI: {question}

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI CHI TIáº¾T:
1. **TÃ³m táº¯t thá»§ tá»¥c**: MÃ´ táº£ ngáº¯n gá»n thá»§ tá»¥c cáº§n thá»±c hiá»‡n
2. **Äiá»u kiá»‡n**: Liá»‡t kÃª rÃµ rÃ ng cÃ¡c Ä‘iá»u kiá»‡n cáº§n Ä‘Ã¡p á»©ng
3. **Há»“ sÆ¡ cáº§n thiáº¿t**: Danh sÃ¡ch Ä‘áº§y Ä‘á»§ giáº¥y tá», sá»‘ lÆ°á»£ng báº£n
4. **Quy trÃ¬nh thá»±c hiá»‡n**: CÃ¡c bÆ°á»›c cá»¥ thá»ƒ theo thá»© tá»±
5. **Thá»i gian vÃ  phÃ­**: Thá»i gian xá»­ lÃ½ vÃ  má»©c phÃ­ (náº¿u cÃ³)

YÃŠU Cáº¦U:
- Sá»­ dá»¥ng ngÃ´n ngá»¯ rÃµ rÃ ng, dá»… hiá»ƒu
- Cáº¥u trÃºc thÃ´ng tin cÃ³ thá»© tá»± logic
- Chá»‰ dá»±a vÃ o thÃ´ng tin trong tÃ i liá»‡u
- Náº¿u thiáº¿u thÃ´ng tin, nÃ³i rÃµ pháº§n nÃ o chÆ°a cÃ³

TRáº¢ Lá»œI:
"""
    else:
        return """
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n thá»§ tá»¥c hÃ nh chÃ­nh. HÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch sÃºc tÃ­ch nhÆ°ng Ä‘áº§y Ä‘á»§.

THÃ”NG TIN THAM KHáº¢O:
{context}

CÃ‚U Há»ŽI: {question}

YÃŠU Cáº¦U TRáº¢ Lá»œI:
- Tráº£ lá»i trá»±c tiáº¿p, ngáº¯n gá»n
- Bao gá»“m thÃ´ng tin chÃ­nh: Ä‘iá»u kiá»‡n, há»“ sÆ¡, quy trÃ¬nh
- Sá»­ dá»¥ng bullet points khi cáº§n thiáº¿t
- Chá»‰ dá»±a trÃªn thÃ´ng tin cÃ³ trong tÃ i liá»‡u

TRáº¢ Lá»œI:
"""

# 5) Generate answer vá»›i Gemini Flash
@st.cache_data(ttl=1800)  # Cache 30 phÃºt
def generate_gemini_answer(query, context, is_detailed, _api_key):
    """Generate answer using Gemini Flash with caching"""
    
    try:
        model = setup_gemini(_api_key)
        
        # Chuáº©n bá»‹ prompt
        prompt_template = get_prompt_template(is_detailed)
        prompt = prompt_template.format(context=context, question=query)
        
        # Generate vá»›i retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                response = model.generate_content(prompt)
                end_time = time.time()
                
                # Log performance
                st.sidebar.success(f"âš¡ Thá»i gian: {end_time - start_time:.2f}s")
                
                return response.text
                
            except Exception as e:
                if attempt < max_retries - 1:
                    st.warning(f"Thá»­ láº¡i láº§n {attempt + 2}...")
                    time.sleep(1)
                else:
                    raise e
                    
    except Exception as e:
        return f"âŒ Lá»—i khi táº¡o cÃ¢u tráº£ lá»i: {str(e)}\n\nVui lÃ²ng kiá»ƒm tra API key hoáº·c thá»­ láº¡i sau."

# 6) Format vÃ  improve answer
def post_process_answer(answer):
    """Post-process Gemini answer for better formatting"""
    
    # Loáº¡i bá» markdown thá»«a
    answer = re.sub(r'\*\*(.*?)\*\*', r'**\1**', answer)
    
    # Cáº£i thiá»‡n format danh sÃ¡ch
    answer = re.sub(r'^\s*[-â€¢]\s*', 'â€¢ ', answer, flags=re.MULTILINE)
    answer = re.sub(r'^\s*(\d+)[\.)]\s*', r'\1. ', answer, flags=re.MULTILINE)
    
    # Chuáº©n hÃ³a line breaks
    answer = re.sub(r'\n{3,}', '\n\n', answer)
    
    # ThÃªm icons cho cÃ¡c section
    answer = re.sub(r'^(#+ .*)', r' \1', answer, flags=re.MULTILINE)
    answer = re.sub(r'\*\*(Äiá»u kiá»‡n|YÃªu cáº§u)([^*]*)\*\*', r' **\1\2**', answer)
    answer = re.sub(r'\*\*(Há»“ sÆ¡|Giáº¥y tá»)([^*]*)\*\*', r' **\1\2**', answer)
    answer = re.sub(r'\*\*(Quy trÃ¬nh|Thá»§ tá»¥c)([^*]*)\*\*', r' **\1\2**', answer)
    answer = re.sub(r'\*\*(Thá»i gian|PhÃ­)([^*]*)\*\*', r' **\1\2**', answer)
    
    return answer.strip()

# 7) Main execution
with st.spinner("Äang tÃ¬m kiáº¿m thÃ´ng tin..."):
    vs = load_vectorstore()
    docs = vs.similarity_search(query, k=top_k)

# 8) Prepare context cho Gemini
if docs:
    context_parts = []
    for i, doc in enumerate(docs, 1):
        # Giá»›i háº¡n Ä‘á»™ dÃ i context Ä‘á»ƒ tá»‘i Æ°u token
        content = doc.page_content[:1000] if not detailed else doc.page_content[:1500]
        source = doc.metadata.get("source", f"TÃ i liá»‡u {i}")
        context_parts.append(f"=== {source} ===\n{content}")
    
    context = "\n\n".join(context_parts)
else:
    context = "KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan."

# 9) Hiá»ƒn thá»‹ tÃ i liá»‡u tham kháº£o
with st.expander("ðŸ“š TÃ i liá»‡u tham kháº£o", expanded=False):
    if docs:
        for i, d in enumerate(docs, 1):
            snippet = d.page_content[:200].strip().replace("\n", " ")
            st.markdown(f"**ðŸ“„ TÃ i liá»‡u {i}:** {snippet}...")
            
            # Metadata
            if d.metadata:
                meta_info = []
                for key, value in d.metadata.items():
                    if key == "url" and value:
                        meta_info.append(f"[ðŸ”— Nguá»“n]({value})")
                    elif key != "url":
                        meta_info.append(f"**{key}:** {value}")
                
                if meta_info:
                    st.markdown(" | ".join(meta_info))
            
            st.divider()
    else:
        st.info("KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan.")

# 10) Generate answer vá»›i Gemini Flash
answer_type = "chi tiáº¿t" if detailed else "nhanh"
with st.spinner(f"ðŸ¤– Gemini Ä‘ang táº¡o cÃ¢u tráº£ lá»i {answer_type}..."):
    
    if docs:
        raw_answer = generate_gemini_answer(
            query, 
            context, 
            detailed, 
            gemini_key
        )
        
        # Post-process
        final_answer = post_process_answer(raw_answer)
        
        # ThÃªm metadata
        final_answer += f"\n\n---\nðŸ”¬ **PhÃ¢n tÃ­ch tá»« {len(docs)} tÃ i liá»‡u** â€¢ ðŸ¤– **Gemini 1.5 Flash** â€¢ âš¡ **Cached 30 phÃºt**"
        
    else:
        final_answer = """
âŒ **KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p**

ðŸ’¡ **Gá»£i Ã½:**
â€¢ Thá»­ tá»« khÃ³a khÃ¡c cá»¥ thá»ƒ hÆ¡n
â€¢ Kiá»ƒm tra chÃ­nh táº£
â€¢ Äáº·t cÃ¢u há»i vá» thá»§ tá»¥c cá»¥ thá»ƒ

ðŸ” **VÃ­ dá»¥ cÃ¢u há»i tá»‘t:**
â€¢ "Thá»§ tá»¥c lÃ m CMND má»›i"
â€¢ "ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬"
â€¢ "Quy trÃ¬nh xin giáº¥y phÃ©p kinh doanh"
"""

# 11) Hiá»ƒn thá»‹ káº¿t quáº£
render_answer(final_answer)

# 12) Action buttons
st.markdown("---")
col1, col2, col3, col4 = st.columns(4)

with col1:
    if st.button("ðŸ‘ Há»¯u Ã­ch", use_container_width=True):
        st.success("Cáº£m Æ¡n feedback!")

with col2:
    if st.button("ðŸ‘Ž Cáº§n cáº£i thiá»‡n", use_container_width=True):
        st.info("ÄÃ£ ghi nháº­n feedback!")

with col3:
    if st.button("ðŸ”„ Táº¡o láº¡i", use_container_width=True):
        st.cache_data.clear()
        st.rerun()

with col4:
    if st.button("ðŸ“‹ Cháº¿ Ä‘á»™ khÃ¡c", use_container_width=True):
        st.info("Thá»­ nÃºt 'Tráº£ lá»i chi tiáº¿t' hoáº·c 'TÃ¬m kiáº¿m' Ä‘á»ƒ thay Ä‘á»•i Ä‘á»™ chi tiáº¿t!")

# 13) Sidebar stats
with st.sidebar:
    st.markdown("---")
    st.markdown("### Thá»‘ng kÃª")
    
    if docs:
        st.metric("TÃ i liá»‡u tÃ¬m tháº¥y", len(docs))
        
        # TÃ­nh Ä‘á»™ liÃªn quan
        query_words = set(query.lower().split())
        total_matches = 0
        for doc in docs:
            doc_words = set(doc.page_content.lower().split())
            matches = len(query_words.intersection(doc_words))
            total_matches += matches
        
        relevance = min(total_matches / len(query_words) / len(docs), 1.0) if query_words else 0
        st.metric("Äá»™ liÃªn quan", f"{relevance:.1%}")
        
        # Token estimation
        context_tokens = len(context.split()) * 1.3  # Rough estimate
        st.metric("Tokens Æ°á»›c tÃ­nh", f"{int(context_tokens)}")
    
    st.markdown("### Gemini Flash")
    st.info("""
    **Æ¯u Ä‘iá»ƒm:**
    â€¢ âš¡ Tá»‘c Ä‘á»™ nhanh
    â€¢ ðŸ’° Chi phÃ­ tháº¥p  
    â€¢ ðŸŽ¯ Cháº¥t lÆ°á»£ng cao
    â€¢ ðŸ§  Context dÃ i (1M tokens)
    """)
    
    st.markdown("### ðŸ’¡ Tips")
    st.success("""
    **CÃ¢u há»i hiá»‡u quáº£:**
    â€¢ Cá»¥ thá»ƒ vÃ  rÃµ rÃ ng
    â€¢ Má»™t thá»§ tá»¥c/váº¥n Ä‘á»
    â€¢ Bao gá»“m ngá»¯ cáº£nh
    
    **VÃ­ dá»¥:** 
    "Thá»§ tá»¥c xin visa du lá»‹ch HÃ n Quá»‘c cho ngÆ°á»i Viá»‡t Nam"
    """)
=======
# 3) TÃ¬m tÃ i liá»‡u tham kháº£o
with st.spinner("TÃ¬m tÃ i liá»‡u tham kháº£oâ€¦"):
    vs = load_vectorstore()
    docs = vs.similarity_search(query, k=3)

# 4) Hiá»ƒn thá»‹ expander tÃ i liá»‡u tham kháº£o
with st.expander("ðŸ“„ TÃ i liá»‡u tham kháº£o", expanded=False):
    for i, d in enumerate(docs, 1):
        snippet_preview = d.page_content[:200].strip().replace("\n", " ")
        st.markdown(f"**TÃ i liá»‡u {i}:** {snippet_preview}â€¦")
        if url := d.metadata.get("url"):
            st.markdown(f"[Nguá»“n]({url})")
        st.divider()

# 5) Láº¥y toÃ n bá»™ ná»™i dung cá»§a doc thá»a mÃ£n query lÃ m cÃ¢u tráº£ lá»i
with st.spinner("Äang trÃ­ch xuáº¥t cÃ¢u tráº£ lá»iâ€¦"):
    answer_parts = []
    q_lower = query.lower()
    for i, d in enumerate(docs, 1):
        content = d.page_content.strip()
        if q_lower in content.lower():
            # ThÃªm toÃ n bá»™ ná»™i dung tÃ i liá»‡u vÃ o cÃ¢u tráº£ lá»i
            answer_parts.append(f"**Theo tÃ i liá»‡u {i}:**\n{content}")
    if not answer_parts:
        answer = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong cÃ¡c tÃ i liá»‡u tham kháº£o."
    else:
        answer = "\n\n".join(answer_parts)

# 6) Hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i
render_answer(answer)
>>>>>>> e10216675b09a9e6e5fe87885184f76e4f60b7a1
