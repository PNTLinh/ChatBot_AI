import streamlit as st
from components import render_header, render_answer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import google.generativeai as genai
import re
import time

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="TTHC RAG System - Gemini Flash",
    layout="wide",
)

render_header()

# Sidebar config
with st.sidebar:
    st.header("ü§ñ Gemini 1.5 Flash")
    gemini_key = st.text_input(
        "Google API Key:", 
        type="password",
        help="L·∫•y API key t·∫°i: https://aistudio.google.com/app/apikey"
    )
    
    # Advanced settings
    with st.expander("‚öôÔ∏è C√†i ƒë·∫∑t n√¢ng cao"):
        temperature = st.slider("Temperature (ƒê·ªô s√°ng t·∫°o)", 0.0, 1.0, 0.1, 0.1)
        max_tokens = st.slider("Max tokens", 100, 2000, 800, 100)
        top_k = st.slider("S·ªë t√†i li·ªáu tham kh·∫£o", 2, 8, 4, 1)

# 1) Form nh·∫≠p c√¢u h·ªèi
with st.form("query_form"):
    query = st.text_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ th·ªß t·ª•c h√†nh ch√≠nh")
    col1, col2 = st.columns(2)
    with col1:
        submitted = st.form_submit_button("T√¨m ki·∫øm", use_container_width=True)
    with col2:
        detailed = st.form_submit_button("Tr·∫£ l·ªùi chi ti·∫øt", use_container_width=True)

if not (submitted or detailed) or not query:
    st.stop()

# Ki·ªÉm tra API key
if not gemini_key:
    st.error(" Vui l√≤ng nh·∫≠p Google API Key trong sidebar")
    st.stop()

# 2) Setup Gemini
@st.cache_resource
def setup_gemini(api_key):
    """Setup Gemini model with caching"""
    genai.configure(api_key=api_key)
    
    # C·∫•u h√¨nh generation
    generation_config = {
        "temperature": temperature,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": max_tokens,
    }
    
    # Safety settings (t√πy ch·ªçn)
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    
    return genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=generation_config,
        safety_settings=safety_settings
    )

# 3) Load vectorstore
@st.cache_resource(ttl=86400)
def load_vectorstore():
    embed = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    return FAISS.load_local(
        "data/faiss_store",
        embed,
        allow_dangerous_deserialization=True,
    )

# 4) Optimized prompts cho Gemini Flash
def get_prompt_template(is_detailed=False):
    """Get optimized prompt template for Gemini Flash"""
    
    if is_detailed:
        return """
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh Vi·ªát Nam v·ªõi kinh nghi·ªám 10 nƒÉm.

TH√îNG TIN THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI CHI TI·∫æT:
1. **T√≥m t·∫Øt th·ªß t·ª•c**: M√¥ t·∫£ ng·∫Øn g·ªçn th·ªß t·ª•c c·∫ßn th·ª±c hi·ªán
2. **ƒêi·ªÅu ki·ªán**: Li·ªát k√™ r√µ r√†ng c√°c ƒëi·ªÅu ki·ªán c·∫ßn ƒë√°p ·ª©ng
3. **H·ªì s∆° c·∫ßn thi·∫øt**: Danh s√°ch ƒë·∫ßy ƒë·ªß gi·∫•y t·ªù, s·ªë l∆∞·ª£ng b·∫£n
4. **Quy tr√¨nh th·ª±c hi·ªán**: C√°c b∆∞·ªõc c·ª• th·ªÉ theo th·ª© t·ª±
5. **Th·ªùi gian v√† ph√≠**: Th·ªùi gian x·ª≠ l√Ω v√† m·ª©c ph√≠ (n·∫øu c√≥)

Y√äU C·∫¶U:
- S·ª≠ d·ª•ng ng√¥n ng·ªØ r√µ r√†ng, d·ªÖ hi·ªÉu
- C·∫•u tr√∫c th√¥ng tin c√≥ th·ª© t·ª± logic
- Ch·ªâ d·ª±a v√†o th√¥ng tin trong t√†i li·ªáu
- N·∫øu thi·∫øu th√¥ng tin, n√≥i r√µ ph·∫ßn n√†o ch∆∞a c√≥

TR·∫¢ L·ªúI:
"""
    else:
        return """
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch s√∫c t√≠ch nh∆∞ng ƒë·∫ßy ƒë·ªß.

TH√îNG TIN THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

Y√äU C·∫¶U TR·∫¢ L·ªúI:
- Tr·∫£ l·ªùi tr·ª±c ti·∫øp, ng·∫Øn g·ªçn
- Bao g·ªìm th√¥ng tin ch√≠nh: ƒëi·ªÅu ki·ªán, h·ªì s∆°, quy tr√¨nh
- S·ª≠ d·ª•ng bullet points khi c·∫ßn thi·∫øt
- Ch·ªâ d·ª±a tr√™n th√¥ng tin c√≥ trong t√†i li·ªáu

TR·∫¢ L·ªúI:
"""

# 5) Generate answer v·ªõi Gemini Flash
@st.cache_data(ttl=1800)  # Cache 30 ph√∫t
def generate_gemini_answer(query, context, is_detailed, _api_key):
    """Generate answer using Gemini Flash with caching"""
    
    try:
        model = setup_gemini(_api_key)
        
        # Chu·∫©n b·ªã prompt
        prompt_template = get_prompt_template(is_detailed)
        prompt = prompt_template.format(context=context, question=query)
        
        # Generate v·ªõi retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                response = model.generate_content(prompt)
                end_time = time.time()
                
                # Log performance
                st.sidebar.success(f"‚ö° Th·ªùi gian: {end_time - start_time:.2f}s")
                
                return response.text
                
            except Exception as e:
                if attempt < max_retries - 1:
                    st.warning(f"Th·ª≠ l·∫°i l·∫ßn {attempt + 2}...")
                    time.sleep(1)
                else:
                    raise e
                    
    except Exception as e:
        return f"‚ùå L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i sau."

# 6) Format v√† improve answer
def post_process_answer(answer):
    """Post-process Gemini answer for better formatting"""
    
    # Lo·∫°i b·ªè markdown th·ª´a
    answer = re.sub(r'\*\*(.*?)\*\*', r'**\1**', answer)
    
    # C·∫£i thi·ªán format danh s√°ch
    answer = re.sub(r'^\s*[-‚Ä¢]\s*', '‚Ä¢ ', answer, flags=re.MULTILINE)
    answer = re.sub(r'^\s*(\d+)[\.)]\s*', r'\1. ', answer, flags=re.MULTILINE)
    
    # Chu·∫©n h√≥a line breaks
    answer = re.sub(r'\n{3,}', '\n\n', answer)
    
    # Th√™m icons cho c√°c section
    answer = re.sub(r'^(#+ .*)', r' \1', answer, flags=re.MULTILINE)
    answer = re.sub(r'\*\*(ƒêi·ªÅu ki·ªán|Y√™u c·∫ßu)([^*]*)\*\*', r' **\1\2**', answer)
    answer = re.sub(r'\*\*(H·ªì s∆°|Gi·∫•y t·ªù)([^*]*)\*\*', r' **\1\2**', answer)
    answer = re.sub(r'\*\*(Quy tr√¨nh|Th·ªß t·ª•c)([^*]*)\*\*', r' **\1\2**', answer)
    answer = re.sub(r'\*\*(Th·ªùi gian|Ph√≠)([^*]*)\*\*', r' **\1\2**', answer)
    
    return answer.strip()

# 7) Main execution
with st.spinner("ƒêang t√¨m ki·∫øm th√¥ng tin..."):
    vs = load_vectorstore()
    docs = vs.similarity_search(query, k=top_k)

# 8) Prepare context cho Gemini
if docs:
    context_parts = []
    for i, doc in enumerate(docs, 1):
        # Gi·ªõi h·∫°n ƒë·ªô d√†i context ƒë·ªÉ t·ªëi ∆∞u token
        content = doc.page_content[:1000] if not detailed else doc.page_content[:1500]
        source = doc.metadata.get("source", f"T√†i li·ªáu {i}")
        context_parts.append(f"=== {source} ===\n{content}")
    
    context = "\n\n".join(context_parts)
else:
    context = "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan."

# 9) Hi·ªÉn th·ªã t√†i li·ªáu tham kh·∫£o
with st.expander("üìö T√†i li·ªáu tham kh·∫£o", expanded=False):
    if docs:
        for i, d in enumerate(docs, 1):
            snippet = d.page_content[:200].strip().replace("\n", " ")
            st.markdown(f"**üìÑ T√†i li·ªáu {i}:** {snippet}...")
            
            # Metadata
            if d.metadata:
                meta_info = []
                for key, value in d.metadata.items():
                    if key == "url" and value:
                        meta_info.append(f"[üîó Ngu·ªìn]({value})")
                    elif key != "url":
                        meta_info.append(f"**{key}:** {value}")
                
                if meta_info:
                    st.markdown(" | ".join(meta_info))
            
            st.divider()
    else:
        st.info("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.")

# 10) Generate answer v·ªõi Gemini Flash
answer_type = "chi ti·∫øt" if detailed else "nhanh"
with st.spinner(f"ü§ñ Gemini ƒëang t·∫°o c√¢u tr·∫£ l·ªùi {answer_type}..."):
    
    if docs:
        raw_answer = generate_gemini_answer(
            query, 
            context, 
            detailed, 
            gemini_key
        )
        
        # Post-process
        final_answer = post_process_answer(raw_answer)
        
        # Th√™m metadata
        final_answer += f"\n\n---\nüî¨ **Ph√¢n t√≠ch t·ª´ {len(docs)} t√†i li·ªáu** ‚Ä¢ ü§ñ **Gemini 1.5 Flash** ‚Ä¢ ‚ö° **Cached 30 ph√∫t**"
        
    else:
        final_answer = """
‚ùå **Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p**

üí° **G·ª£i √Ω:**
‚Ä¢ Th·ª≠ t·ª´ kh√≥a kh√°c c·ª• th·ªÉ h∆°n
‚Ä¢ Ki·ªÉm tra ch√≠nh t·∫£
‚Ä¢ ƒê·∫∑t c√¢u h·ªèi v·ªÅ th·ªß t·ª•c c·ª• th·ªÉ

üîç **V√≠ d·ª• c√¢u h·ªèi t·ªët:**
‚Ä¢ "Th·ªß t·ª•c l√†m CMND m·ªõi"
‚Ä¢ "ƒêƒÉng k√Ω k·∫øt h√¥n c·∫ßn gi·∫•y t·ªù g√¨"
‚Ä¢ "Quy tr√¨nh xin gi·∫•y ph√©p kinh doanh"
"""

# 11) Hi·ªÉn th·ªã k·∫øt qu·∫£
render_answer(final_answer)

# 12) Action buttons
st.markdown("---")
col1, col2, col3, col4 = st.columns(4)

with col1:
    if st.button("üëç H·ªØu √≠ch", use_container_width=True):
        st.success("C·∫£m ∆°n feedback!")

with col2:
    if st.button("üëé C·∫ßn c·∫£i thi·ªán", use_container_width=True):
        st.info("ƒê√£ ghi nh·∫≠n feedback!")

with col3:
    if st.button("üîÑ T·∫°o l·∫°i", use_container_width=True):
        st.cache_data.clear()
        st.rerun()

with col4:
    if st.button("üìã Ch·∫ø ƒë·ªô kh√°c", use_container_width=True):
        st.info("Th·ª≠ n√∫t 'Tr·∫£ l·ªùi chi ti·∫øt' ho·∫∑c 'T√¨m ki·∫øm' ƒë·ªÉ thay ƒë·ªïi ƒë·ªô chi ti·∫øt!")

# 13) Sidebar stats
with st.sidebar:
    st.markdown("---")
    st.markdown("### Th·ªëng k√™")
    
    if docs:
        st.metric("T√†i li·ªáu t√¨m th·∫•y", len(docs))
        
        # T√≠nh ƒë·ªô li√™n quan
        query_words = set(query.lower().split())
        total_matches = 0
        for doc in docs:
            doc_words = set(doc.page_content.lower().split())
            matches = len(query_words.intersection(doc_words))
            total_matches += matches
        
        relevance = min(total_matches / len(query_words) / len(docs), 1.0) if query_words else 0
        st.metric("ƒê·ªô li√™n quan", f"{relevance:.1%}")
        
        # Token estimation
        context_tokens = len(context.split()) * 1.3  # Rough estimate
        st.metric("Tokens ∆∞·ªõc t√≠nh", f"{int(context_tokens)}")
    
    st.markdown("### Gemini Flash")
    st.info("""
    **∆Øu ƒëi·ªÉm:**
    ‚Ä¢ ‚ö° T·ªëc ƒë·ªô nhanh
    ‚Ä¢ üí∞ Chi ph√≠ th·∫•p  
    ‚Ä¢ üéØ Ch·∫•t l∆∞·ª£ng cao
    ‚Ä¢ üß† Context d√†i (1M tokens)
    """)
    
    st.markdown("### üí° Tips")
    st.success("""
    **C√¢u h·ªèi hi·ªáu qu·∫£:**
    ‚Ä¢ C·ª• th·ªÉ v√† r√µ r√†ng
    ‚Ä¢ M·ªôt th·ªß t·ª•c/v·∫•n ƒë·ªÅ
    ‚Ä¢ Bao g·ªìm ng·ªØ c·∫£nh
    
    **V√≠ d·ª•:** 
    "Th·ªß t·ª•c xin visa du l·ªãch H√†n Qu·ªëc cho ng∆∞·ªùi Vi·ªát Nam"
    """)