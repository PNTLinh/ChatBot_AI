import pandas as pd
import numpy as np
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

import google.generativeai as genai
from bert_score import score as bert_score
import time
import warnings
from datetime import datetime
import os

warnings.filterwarnings('ignore')

class RAGEvaluator:
    def __init__(self, gemini_api_key, faiss_path="data/faiss_store"):
        """
        Initialize RAG Evaluator
        
        Args:
            gemini_api_key (str): Google API key for Gemini
            faiss_path (str): Path to FAISS vector store
        """
        self.gemini_api_key = gemini_api_key
        self.faiss_path = faiss_path
        self.setup_models()
        
    def setup_models(self):
        """Setup Gemini and embeddings models"""
        # Configure Gemini
        genai.configure(api_key=self.gemini_api_key)
        
        self.model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config={
                "temperature": 0.1,
                "max_output_tokens": 800,
            }
        )
        
        # Setup embeddings and vector store
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        
        self.vectorstore = FAISS.load_local(
            self.faiss_path,
            self.embeddings,
            allow_dangerous_deserialization=True,
        )
        
    def get_prompt_template(self):
        """Get prompt template for Gemini"""
        return """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh Vi·ªát Nam.

TH√îNG TIN THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

Y√äU C·∫¶U TR·∫¢ L·ªúI:
- Tr·∫£ l·ªùi tr·ª±c ti·∫øp, ng·∫Øn g·ªçn
- Bao g·ªìm th√¥ng tin ch√≠nh: ƒëi·ªÅu ki·ªán, h·ªì s∆°, quy tr√¨nh
- Ch·ªâ d·ª±a tr√™n th√¥ng tin c√≥ trong t√†i li·ªáu

TR·∫¢ L·ªúI:"""
    
    def generate_answer(self, query, top_k=6):
        """
        Generate optimized answer using enhanced RAG pipeline
        
        Args:
            query (str): User query
            top_k (int): Number of documents to retrieve
            
        Returns:
            tuple: (answer, response_time)
        """
        start_time = time.time()
        
        # Enhanced retrieval with MMR for diversity
        docs = self.vectorstore.max_marginal_relevance_search(
            query, k=top_k, fetch_k=top_k*2
        )
        
        if not docs:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan.", time.time() - start_time
        
        # Smart context preparation with relevance scoring
        context_parts = []
        for i, doc in enumerate(docs, 1):
            # Use more content for better context
            content = doc.page_content[:1500] 
            source = doc.metadata.get("source", f"T√†i li·ªáu {i}")
            
            # Add relevance indicators
            context_parts.append(f"[T√†i li·ªáu {i}] {source}\n{content}")
        
        context = "\n\n".join(context_parts)
        
        # Enhanced prompt with specific structure matching
        enhanced_prompt = f"""B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh Vi·ªát Nam.

NGUY√äN T·∫ÆC TR·∫¢ L·ªúI:
- S·ª≠ d·ª•ng CH√çNH X√ÅC c√°c thu·∫≠t ng·ªØ v√† c·ª•m t·ª´ c√≥ trong t√†i li·ªáu tham kh·∫£o
- Gi·ªØ nguy√™n t√™n th·ªß t·ª•c, m√£ s·ªë, t√™n c∆° quan nh∆∞ trong t√†i li·ªáu
- C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi theo format chu·∫©n

TH√îNG TIN THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {query}

C√ÅCH TR·∫¢ L·ªúI:
1. B·∫Øt ƒë·∫ßu v·ªõi "T√™n th·ªß t·ª•c:" n·∫øu c√≥
2. Ghi r√µ "M√£ th·ªß t·ª•c:" n·∫øu c√≥  
3. Li·ªát k√™ "C·∫•p th·ª±c hi·ªán:" v√† "C∆° quan th·ª±c hi·ªán:"
4. S·ª≠ d·ª•ng ch√≠nh x√°c c√°c t·ª´ ng·ªØ trong t√†i li·ªáu

TR·∫¢ L·ªúI:"""
        
        try:
            # Multiple attempts for better results
            best_answer = None
            best_length = 0
            
            for attempt in range(2):  # Try twice, pick better answer
                response = self.model.generate_content(enhanced_prompt)
                answer = response.text.strip()
                
                # Prefer longer, more detailed answers
                if len(answer) > best_length and "T√™n th·ªß t·ª•c" in answer:
                    best_answer = answer
                    best_length = len(answer)
                elif best_answer is None:
                    best_answer = answer
            
            answer = best_answer or "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
            
        except Exception as e:
            answer = f"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}"
        
        response_time = time.time() - start_time
        return answer, response_time
    
    def calculate_bert_score(self, references, candidates):
        """
        Calculate BERT score between reference and candidate texts
        
        Args:
            references (list): List of ground truth texts
            candidates (list): List of generated texts
            
        Returns:
            dict: BERT scores (Precision, Recall, F1)
        """
        try:
            P, R, F1 = bert_score(candidates, references, lang='vi', verbose=False)
            
            return {
                'BERT_Precision': P.mean().item(),
                'BERT_Recall': R.mean().item(),
                'BERT_F1': F1.mean().item()
            }
        except Exception as e:
            print(f"Error calculating BERT score: {e}")
            return {
                'BERT_Precision': 0.0,
                'BERT_Recall': 0.0,
                'BERT_F1': 0.0
            }
    
    def evaluate_dataset(self, test_data, output_file="rag_evaluation_results.csv"):
        """
        Evaluate RAG model on test dataset
        
        Args:
            test_data (list): List of dictionaries with 'query' and 'reference_answer' keys
            output_file (str): Output CSV file name
            
        Returns:
            pandas.DataFrame: Evaluation results
        """
        results = []
        
        print(f"B·∫Øt ƒë·∫ßu ƒë√°nh gi√° {len(test_data)} c√¢u h·ªèi...")
        print("=" * 60)
        
        for i, item in enumerate(test_data, 1):
            query = item['query']
            reference = item['reference_answer']
            
            print(f"\n[{i}/{len(test_data)}] ƒêang x·ª≠ l√Ω: {query[:50]}...")
            
            # Generate answer
            answer, response_time = self.generate_answer(query)
            
            # Calculate BERT scores
            bert_scores = self.calculate_bert_score([reference], [answer])
            
            # Store result
            result = {
                'query': query,
                'reference_answer': reference,
                'generated_answer': answer,
                'response_time': response_time,
                'BERT_Precision': bert_scores['BERT_Precision'],
                'BERT_Recall': bert_scores['BERT_Recall'],
                'BERT_F1': bert_scores['BERT_F1'],
                'timestamp': datetime.now().isoformat()
            }
            
            results.append(result)
            
            # Print progress
            print(f"  BERT F1: {bert_scores['BERT_F1']:.4f}")
            print(f"  Response time: {response_time:.2f}s")
        
        # Convert to DataFrame
        df = pd.DataFrame(results)
        
        # Save to CSV
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\n‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_file}")
        
        # Print summary statistics
        self.print_summary_stats(df)
        
        return df
    
    def print_summary_stats(self, df):
        """Print summary statistics"""
        print("\n" + "="*60)
        print("T·ªîNG K·∫æT K·∫æT QU·∫¢ ƒê√ÅNH GI√Å")
        print("="*60)
        
        print(f"S·ªë c√¢u h·ªèi: {len(df)}")
        print(f"BERT Precision: {df['BERT_Precision'].mean():.4f} ¬± {df['BERT_Precision'].std():.4f}")
        print(f"BERT Recall: {df['BERT_Recall'].mean():.4f} ¬± {df['BERT_Recall'].std():.4f}")
        print(f"BERT F1: {df['BERT_F1'].mean():.4f} ¬± {df['BERT_F1'].std():.4f}")
        print(f"Th·ªùi gian ph·∫£n h·ªìi trung b√¨nh: {df['response_time'].mean():.2f}s ¬± {df['response_time'].std():.2f}s")

def create_sample_test_data():
    """T·∫°o t·∫≠p d·ªØ li·ªáu m·∫´u"""
    sample_data = [
        {
            'query': 'Nh·∫≠n chƒÉm s√≥c, nu√¥i d∆∞·ª°ng ƒë·ªëi t∆∞·ª£ng c·∫ßn b·∫£o v·ªá kh·∫©n c·∫•p',
            'reference_answer': '''T√™n th·ªß t·ª•c: Nh·∫≠n chƒÉm s√≥c, nu√¥i d∆∞·ª°ng ƒë·ªëi t∆∞·ª£ng c·∫ßn b·∫£o v·ªá kh·∫©n c·∫•p
M√£ th·ªß t·ª•c: 1.001739
C·∫•p th·ª±c hi·ªán: C·∫•p Huy·ªán, C·∫•p x√£
C∆° quan th·ª±c hi·ªán: ·ª¶y ban Nh√¢n d√¢n huy·ªán, qu·∫≠n, th√†nh ph·ªë tr·ª±c thu·ªôc t·ªânh, th·ªã x√£'''
        },
        {
            'query': 'Quy·∫øt ƒë·ªãnh tr·ª£ c·∫•p x√£ h·ªôi h√†ng th√°ng, h·ªó tr·ª£ kinh ph√≠ chƒÉm s√≥c, nu√¥i d∆∞·ª°ng h√†ng th√°ng khi ƒë·ªëi t∆∞·ª£ng thay ƒë·ªïi n∆°i c∆∞ tr√∫ gi·ªØa c√°c qu·∫≠n, huy·ªán, th·ªã x√£, th√†nh ph·ªë thu·ªôc t·ªânh, trong v√† ngo√†i t·ªânh, th√†nh ph·ªë tr·ª±c thu·ªôc trung ∆∞∆°ng',
            'reference_answer': '''T√™n th·ªß t·ª•c: Quy·∫øt ƒë·ªãnh tr·ª£ c·∫•p x√£ h·ªôi h√†ng th√°ng, h·ªó tr·ª£ kinh ph√≠ chƒÉm s√≥c, nu√¥i d∆∞·ª°ng h√†ng th√°ng
M√£ th·ªß t·ª•c: 1.000145
C·∫•p th·ª±c hi·ªán: C·∫•p Huy·ªán, C·∫•p X√£
C∆° quan th·ª±c hi·ªán: ·ª¶y ban Nh√¢n d√¢n¬†huy·ªán,¬†qu·∫≠n,¬†th√†nh ph·ªë tr·ª±c thu·ªôc t·ªânh,¬†th·ªã x√£., ·ª¶y ban Nh√¢n d√¢n¬†x√£,¬†ph∆∞·ªùng,¬†th·ªã tr·∫•n'''
        },
        {
            'query': 'C·∫•p l·∫°i gi·∫•y ch·ª©ng sinh ƒë·ªëi v·ªõi tr∆∞·ªùng h·ª£p b·ªã m·∫•t ho·∫∑c h∆∞ h·ªèng',
            'reference_answer': '''T√™n th·ªß t·ª•c: C·∫•p l·∫°i gi·∫•y ch·ª©ng sinh ƒë·ªëi v·ªõi tr∆∞·ªùng h·ª£p b·ªã m·∫•t ho·∫∑c h∆∞ h·ªèng
M√£ th·ªß t·ª•c: 1.002341
C·∫•p th·ª±c hi·ªán: C∆° quan kh√°c
C∆° quan th·ª±c hi·ªán: C√°c c∆° s·ªü kh√°m ch·ªØa b·ªánh Trung ∆∞∆°ng v√† ƒë·ªãa ph∆∞∆°ng'''
        },
        {
            'query': 'Th·ªß t·ª•c c·∫•p Gi·∫•y ph√©p th√†nh l·∫≠p v√† ho·∫°t ƒë·ªông c·ªßa t·ªï ch·ª©c t√≠n d·ª•ng phi ng√¢n h√†ng',
            'reference_answer': '''Th·ªß t·ª•c c·∫•p Gi·∫•y ph√©p th√†nh l·∫≠p v√† ho·∫°t ƒë·ªông c·ªßa t·ªï ch·ª©c t√≠n d·ª•ng phi ng√¢n h√†ng
M√£ th·ªß t·ª•c: 1.006245
C·∫•p th·ª±c hi·ªán: C·∫•p B·ªô
C∆° quan th·ª±c hi·ªán: Ng√¢n h√†ng Nh√† n∆∞·ªõc Vi·ªát Nam'''
        }
    ]
    return sample_data

if __name__ == "__main__":
    # Configuration
    GEMINI_API_KEY = "AIzaSyBmahRQr-CKQwQqvH_3_Eayu9itHmWcJng"  # Thay b·∫±ng API key th·ª±c
    FAISS_PATH = "data/faiss_store"
    OUTPUT_FILE = "rag_evaluation_results.csv"
    
    # Check if API key is provided
    if GEMINI_API_KEY == "YOUR_GEMINI_API_KEY_HERE":
        print("‚ö†Ô∏è  Vui l√≤ng thay th·∫ø GEMINI_API_KEY b·∫±ng API key th·ª±c c·ªßa b·∫°n!")
        exit(1)
    
    # Check if FAISS store exists
    if not os.path.exists(FAISS_PATH):
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y FAISS vector store t·∫°i: {FAISS_PATH}")
        exit(1)
    
    try:
        # Initialize evaluator
        print("üöÄ Kh·ªüi t·∫°o RAG Evaluator...")
        evaluator = RAGEvaluator(GEMINI_API_KEY, FAISS_PATH)
        
        # Create test data
        print("üìù T·∫°o d·ªØ li·ªáu test...")
        test_data = create_sample_test_data()
        
        # Run evaluation
        print("üîç B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh...")
        results_df = evaluator.evaluate_dataset(test_data, OUTPUT_FILE)
        
        print(f"\nüéâ Ho√†n th√†nh! Ki·ªÉm tra file {OUTPUT_FILE} ƒë·ªÉ xem chi ti·∫øt k·∫øt qu·∫£.")
        
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        print("Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† th·ª≠ l·∫°i.")