import streamlit as st  
from dotenv import load_dotenv  
from seed_data import seed_milvus, seed_milvus_live
from agent import get_retriever as get_openai_retriever, get_llm_and_agent as get_openai_agent
from local_ollama import get_retriever as get_ollama_retriever, get_llm_and_agent as get_ollama_agent
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

def setup_page():
    st.set_page_config(
        page_title="AI Assistant",  
        page_icon="üí¨",  
        layout="wide"  
    )

def init_app():
    load_dotenv() 
    setup_page()  
    setup_sidebar()
    
def setup_sidebar():
    with st.sidebar:
        st.title("Options")
        st.subheader("Settings")
        data_src = st.radio("Select data source", ("Local file", "Web URL"))
        if data_src == "Local file":
            filename  = st.text_input("Enter file name")
            directory = st.text_input("Enter directory")
            if st.button("Load file"):
                with st.spinner("Loading file..."):
                    seed_milvus('https://localhost:19530', 'data_test',filename, directory)
                st.success("File loaded successfully")
        else:
            url = st.text_input("Enter URL")
            if st.button("Crawl URL"):
                with st.spinner("Crawling URL..."):
                    seed_milvus('https://localhost:19530', 'data_test',url)
                st.success("URL crawled successfully")

def handle_local_file(use_ollama_embeddings: bool):
    collection_name = st.text_input(
        "T√™n collection trong Milvus:", 
        "data_test",
        help="Nh·∫≠p t√™n collection b·∫°n mu·ªën l∆∞u trong Milvus"
    )
    filename = st.text_input("T√™n file JSON:", "stack.json")
    directory = st.text_input("Th∆∞ m·ª•c ch·ª©a file:", "data")
    
    if st.button("T·∫£i d·ªØ li·ªáu t·ª´ file"):
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
            try:
                seed_milvus(
                    'http://localhost:19530', 
                    collection_name, 
                    filename, 
                    directory, 
                    use_ollama=use_ollama_embeddings
                )
                st.success(f"ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}")

def handle_url_input(use_ollama_embeddings: bool):
    collection_name = st.text_input(
        "T√™n collection trong Milvus:", 
        "data_test_live",
        help="Nh·∫≠p t√™n collection b·∫°n mu·ªën l∆∞u trong Milvus"
    )
    url = st.text_input("Nh·∫≠p URL:", "https://www.stack-ai.com/docs")
    
    if st.button("Crawl d·ªØ li·ªáu"):
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        with st.spinner("ƒêang crawl d·ªØ li·ªáu..."):
            try:
                seed_milvus_live(
                    url, 
                    'http://localhost:19530', 
                    collection_name, 
                    'stack-ai', 
                    use_ollama=use_ollama_embeddings
                )
                st.success(f"ƒê√£ crawl d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
            except Exception as e:
                st.error(f"L·ªói khi crawl d·ªØ li·ªáu: {str(e)}")

def setup_chat_interface(model_choice):
    st.title("üí¨ AI Assistant")
    st.caption("üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† OpenAI GPT-4")
    msgs = StreamlitChatMessageHistory(key="langchain_messages")
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}
        ]
        msgs.add_ai_message("T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?")

    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs

def handle_user_input(msgs, agent_executor):
    if prompt := st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Stack AI!"):
        st.session_state.messages.append({"role": "human", "content": prompt})
        st.chat_message("human").write(prompt)
        msgs.add_user_message(prompt)

        with st.chat_message("assistant"):
            st_callback = StreamlitCallbackHandler(st.container())
            
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]

            response = agent_executor.invoke(
                {
                    "input": prompt,
                    "chat_history": chat_history
                },
                {"callbacks": [st_callback]}
            )
            output = response["output"]
            st.session_state.messages.append({"role": "assistant", "content": output})
            msgs.add_ai_message(output)
            st.write(output)

def main():
    init_app()
    model_choice, collection_to_query = setup_sidebar()
    msgs = setup_chat_interface(model_choice)
    retriever = get_openai_retriever(collection_to_query)
    agent_executor = get_openai_agent(retriever, "gpt4")
    handle_user_input(msgs, agent_executor)

if __name__ == "__main__":
    main() 